{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc94538-379c-448a-ab20-fcb478966d64",
   "metadata": {},
   "source": [
    "<h1>Step: 1- Building a Custom Decision Tree with Information Gain:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5195edf-ce16-4293-baec-f2efc1b97642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Built Decision Tree:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initializes the decision tree with the specified maximum depth.\n",
    "        Parameters:\n",
    "        max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until all\n",
    "        leaves are pure or contain fewer than the minimum samples required to split.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the decision tree model using the provided training data.\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for training the model.\n",
    "        y (array-like): Target labels (n_samples,) for training the model.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds the decision tree by splitting the data based on the best feature and threshold.\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for splitting.\n",
    "        y (array-like): Target labels (n_samples,) for splitting.\n",
    "        depth (int, optional): Current depth of the tree during recursion.\n",
    "        \n",
    "        Returns:\n",
    "        dict: A dictionary representing the structure of the tree, containing the best feature index, threshold, and recursive tree nodes.\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Stopping conditions\n",
    "        if len(unique_classes) == 1:\n",
    "            return {'class': unique_classes[0]}\n",
    "\n",
    "        if num_samples == 0 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "\n",
    "        best_info_gain = -float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                info_gain = self._information_gain(y, left_y, right_y)\n",
    "\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'left_mask': left_mask,\n",
    "                        'right_mask': right_mask\n",
    "                    }\n",
    "\n",
    "        if best_split is None:\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "\n",
    "        # recursion (now reachable)\n",
    "        left_tree = self._build_tree(\n",
    "            X[best_split['left_mask']],\n",
    "            y[best_split['left_mask']],\n",
    "            depth + 1\n",
    "        )\n",
    "\n",
    "        right_tree = self._build_tree(\n",
    "            X[best_split['right_mask']],\n",
    "            y[best_split['right_mask']],\n",
    "            depth + 1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'feature_idx': best_split['feature_idx'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left_tree': left_tree,\n",
    "            'right_tree': right_tree\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Computes the Information Gain between the parent node and the left/right child nodes.\n",
    "        Parameters:\n",
    "        parent (array-like): The labels of the parent node.\n",
    "        left (array-like): The labels of the left child node.\n",
    "        right (array-like): The labels of the right child node.\n",
    "        Returns:\n",
    "        float: The Information Gain of the split.\n",
    "        \"\"\"\n",
    "        parent_entropy = self._entropy(parent)\n",
    "        left_entropy = self._entropy(left)\n",
    "        right_entropy = self._entropy(right)\n",
    "    \n",
    "        # Information Gain = Entropy(parent) - (weighted average of left and right entropies)\n",
    "        weighted_avg_entropy = (len(left) / len(parent)) * left_entropy + (len(right) / len(parent)) * right_entropy\n",
    "        return parent_entropy - weighted_avg_entropy\n",
    "\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of labels.\n",
    "        Parameters:\n",
    "        y (array-like): The labels for which entropy is calculated.\n",
    "        Returns:\n",
    "        float: The entropy of the labels.\n",
    "        \"\"\"\n",
    "        # Calculate the probability of each class\n",
    "        class_probs = np.bincount(y) / len(y)\n",
    "    \n",
    "        # Compute the entropy using the formula: -sum(p * log2(p))\n",
    "        return -np.sum(class_probs * np.log2(class_probs + 1e-9)) # Added small epsilon to avoid log(0)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the target labels for the given test data based on the trained decision tree.\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for prediction.\n",
    "        Returns:\n",
    "        list: A list of predicted target labels (n_samples,).\n",
    "        \"\"\"\n",
    "        return [self._predict_single(x, self.tree) for x in X]\n",
    "\n",
    "        \n",
    "    def _predict_single(self, x, tree):\n",
    "        \"\"\"\n",
    "        Recursively predicts the target label for a single sample by traversing the tree.\n",
    "        Parameters:\n",
    "        x (array-like): A single feature vector for prediction.\n",
    "        tree (dict): The current subtree or node to evaluate.\n",
    "        Returns:\n",
    "        int: The predicted class label for the sample.\n",
    "        \"\"\"\n",
    "        if 'class' in tree:\n",
    "            return tree['class']\n",
    "        feature_val = x[tree['feature_idx']]\n",
    "        if feature_val <= tree['threshold']:\n",
    "            return self._predict_single(x, tree['left_tree'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ab733-c899-450e-b9e4-7e1fefc92a95",
   "metadata": {},
   "source": [
    "<h1>Step: 2- Load and Split the Iris Datasets:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d8ea996-3314-4f2f-883c-fca5d43293b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Split the IRIS Dataset:\n",
    "\n",
    "# Necessary Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# Split into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ad399-2651-4940-a7fa-7f0f3e451280",
   "metadata": {},
   "source": [
    "<h1>Step: 3- Train and Evaluate a Custom Decision Tree:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a0135c-5fb0-47bf-9263-bec3b88e94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train and Evaluate a Custom Decision Tree:\n",
    "\n",
    "# Train the custom decision tree\n",
    "custom_tree = CustomDecisionTree(max_depth=3)\n",
    "custom_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_custom = custom_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4c5b5-dbc3-4956-996e-0a7329fb530d",
   "metadata": {},
   "source": [
    "<h1>Step: 4- Train and Evaluate a Scikit Learn Decision Tree:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c02eb37-5b3e-41da-b5be-2fb292efa3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Train and Evaluate a Scikit Learn Decision Tree:\n",
    "\n",
    "# Train the Scikit-learn decision tree\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "# Predict on the test set\n",
    "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aab890-5b36-4812-b018-24439e133c43",
   "metadata": {},
   "source": [
    "<h1>Step: 5- Result Comparison:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4c4640-557b-4766-964c-613ceb042076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Comparison:\n",
      "Custom Decision Tree: 1.0000\n",
      "Scikit-learn Decision Tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Result Comparision:\n",
    "\n",
    "print(f\"Accuracy Comparison:\")\n",
    "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
    "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce3bea-e8fd-4ea3-8cf5-da27084c2b6d",
   "metadata": {},
   "source": [
    "<h1>Exercise - Ensemble Methods and Hyperparameter Tuning.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "544b6962-5c0c-4e26-8aa8-73a255032602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (178, 13)\n",
      "Unique classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Using the Wine Dataset from scikit-learn\n",
    "# 1. Implement Classification Models:\n",
    "# • Train a Decision Tree Classifier and a Random Forest Classifier using scikit-learn.\n",
    "# • Compare the models based on their F1 scores.\n",
    "\n",
    "# 2. Hyperparameter Tuning:\n",
    "# • Identify three hyperparameters of the Random Forest Classifier.\n",
    "# • Perform hyperparameter tuning using GridSearchCV to optimize these parameters.\n",
    "# • Take hints from the scikit-learn documentation to guide the implementation.\n",
    "\n",
    "# 3. Implement Regression Model:\n",
    "# • Train a Decision Tree Regressor and a Random Forest Regressor using scikit-learn.\n",
    "# • Identify three parameters for Random Forest Regressio and Perform hyperparameter tuning using\n",
    "# RandomSearchCV to optimize these parameters.\n",
    "\n",
    "# Load Dataset & Split\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine(as_frame=True)\n",
    "X = wine.data\n",
    "y_class = wine.target  # For classification (3 classes)\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Unique classes:\", np.unique(y_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c11416-97ab-4bc7-80d3-1879b3981c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Score: 0.945\n",
      "Random Forest F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 1. Classification: Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "dt_f1 = f1_score(y_test, dt_pred, average='weighted')\n",
    "print(f\"Decision Tree F1 Score: {dt_f1:.3f}\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_f1 = f1_score(y_test, rf_pred, average='weighted')\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5963afda-0671-4956-bd40-d674779ca845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Classifier Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best CV F1 Score: 0.986\n"
     ]
    }
   ],
   "source": [
    "# 2. Hyperparameter Tuning for Random Forest Classifier (GridSearchCV)\n",
    "# Three hyperparameters: n_estimators, max_depth, min_samples_split\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Classifier Params:\", grid_search.best_params_)\n",
    "print(\"Best CV F1 Score:\", f\"{grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb40956-435f-427d-a57f-d1a1e0fa51a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 52342.58\n",
      "Random Forest MSE: 29218.06\n"
     ]
    }
   ],
   "source": [
    "# 3. Regression: Use 'proline' as continuous target, drop it from features\n",
    "y_reg = X['proline']\n",
    "X_reg = X.drop('proline', axis=1)\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train_r, y_train_r)\n",
    "dt_pred_r = dt_reg.predict(X_test_r)\n",
    "dt_mse = mean_squared_error(y_test_r, dt_pred_r)\n",
    "print(f\"Decision Tree MSE: {dt_mse:.2f}\")\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "rf_reg.fit(X_train_r, y_train_r)\n",
    "rf_pred_r = rf_reg.predict(X_test_r)\n",
    "rf_mse = mean_squared_error(y_test_r, rf_pred_r)\n",
    "print(f\"Random Forest MSE: {rf_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e1c15a-f9b8-4ec2-8fc2-7e021d379ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Regressor Params: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': 10}\n",
      "Best CV MSE: 30608.61\n"
     ]
    }
   ],
   "source": [
    "# 4. Hyperparameter Tuning for Random Forest Regressor (RandomizedSearchCV)\n",
    "# Three parameters: n_estimators, max_depth, min_samples_split\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train_r, y_train_r)\n",
    "print(\"Best Regressor Params:\", random_search.best_params_)\n",
    "print(\"Best CV MSE:\", f\"{random_search.best_score_ * -1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d68317-598f-4f06-b08c-5dfa112d2ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
